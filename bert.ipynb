{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy\n",
    "# ! pip install zipfile36\n",
    "# ! pip install pandas\n",
    "# ! pip install tqdm\n",
    "# ! pip install ipywidgets\n",
    "# ! pip install scrapbook\n",
    "# ! pip install tempfile\n",
    "# ! pip install tensorflow\n",
    "# ! pip install retrying\n",
    "# ! pip install transformers\n",
    "# ! conda install numpy-base\n",
    "# ! pip install recommenders[npa,gpu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.11 (default, Nov 27 2020, 18:37:51) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensorflow version: 1.15.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')  # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.npa import NPAModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "# Options: demo, small, large\n",
    "MIND_type = 'demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "# data_path = tmpdir.name\n",
    "data_path = os.path.join('datasets', MIND_type)\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
    "wordDict_file_bert = os.path.join(data_path, \"utils\", \"word_dict_bert.pkl\")\n",
    "wordEmb_file_bert = os.path.join(data_path, \"utils\", \"embedding_bert.npy\")\n",
    "\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'npa.yaml')\n",
    "\n",
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "\n",
    "if not os.path.exists(train_news_file):\n",
    "    download_deeprec_resources(mind_url, os.path.join(\n",
    "        data_path, 'train'), mind_train_dataset)\n",
    "\n",
    "if not os.path.exists(valid_news_file):\n",
    "    download_deeprec_resources(mind_url,\n",
    "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/newsrec/',\n",
    "                               os.path.join(data_path, 'utils'), mind_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = ['news_ID', 'category', 'SubCategory', 'title',\n",
    "               'abstract', 'URL', 'titleEntities', 'abstractEntities']\n",
    "news = pd.read_csv(train_news_file , sep='\\t',\n",
    "                 header=None, names=columnNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForPreTraining\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForPreTraining.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26740/26740 [00:34<00:00, 768.99it/s] \n",
      "100%|██████████| 26740/26740 [00:01<00:00, 14006.69it/s]\n",
      "100%|██████████| 26740/26740 [00:24<00:00, 1097.40it/s]\n",
      "100%|██████████| 26740/26740 [00:13<00:00, 1916.15it/s]\n",
      "100%|██████████| 17203/17203 [00:00<00:00, 1076597.06it/s]\n",
      "100%|██████████| 17203/17203 [00:00<00:00, 238934.42it/s]\n"
     ]
    }
   ],
   "source": [
    "wordDictTokens = []\n",
    "for i, title in enumerate(tqdm(news['title'])):\n",
    "    wordDictTokens.append(tokenizer(str(title), truncation=True, padding=True, return_tensors='pt' ))\n",
    "\n",
    "decodedtokens = []\n",
    "for j in tqdm(range(len(wordDictTokens))):\n",
    "    decodedtokens.append(tokenizer.decode(wordDictTokens[j]['input_ids'][0].detach().numpy()))\n",
    "    \n",
    "tokens = []\n",
    "for k, title in enumerate(tqdm(decodedtokens)):\n",
    "    tokens.append(tokenizer.tokenize(str(title)))\n",
    "\n",
    "tokenDF = []\n",
    "for l in tqdm(range(len(tokens))):\n",
    "    tokenDF.append(pd.DataFrame(tokens[l], columns=['words']))\n",
    "\n",
    "uniqTokens = pd.concat(tokenDF).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "tList= uniqTokens['words'].values.tolist()\n",
    "\n",
    "wordDict = []\n",
    "for m in tqdm(range(len(tList))):\n",
    "    wordDict.append({tList[m]: m+1})\n",
    "\n",
    "token_ids = []\n",
    "for n, word in enumerate(tqdm(wordDict)):\n",
    "    token_ids.append(tokenizer.convert_tokens_to_ids(word))\n",
    "\n",
    "with open(wordDict_file_bert, 'wb') as handle:\n",
    "    pickle.dump(wordDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17203/17203 [2:32:35<00:00,  1.88it/s]  \n"
     ]
    }
   ],
   "source": [
    "encoded_input  = []\n",
    "\n",
    "for o, word in enumerate(tqdm(tList)):\n",
    "    # for p, word in enumerate(title):\n",
    "    tokenInputes = tokenizer(\n",
    "        str(word),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=768) \n",
    "    # print(word)\n",
    "    out = model(**tokenInputes)\n",
    "    encoded_input.append(out['last_hidden_state'][0][1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordEmb_file_bert = os.path.join(data_path, \"utils\", \"embedding_bert.npy\")\n",
    "with open(wordEmb_file_bert, 'wb') as f: \n",
    "    np.save(f, encoded_input, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71301967, -0.1801134 , -0.73847884, ..., -0.09353121,\n",
       "         0.31587684,  0.01532312],\n",
       "       [-0.5383994 , -0.78006506, -0.27009398, ...,  0.19941267,\n",
       "         0.31292754, -0.36979085],\n",
       "       [-0.28484663, -0.5982345 ,  0.0042835 , ...,  0.2716275 ,\n",
       "         0.37806424, -0.1773692 ],\n",
       "       ...,\n",
       "       [ 0.07916661, -0.22728246, -0.00243599, ...,  0.51640457,\n",
       "         0.2333358 , -0.14949359],\n",
       "       [-0.0062239 , -0.29653686,  0.12724929, ...,  0.53706366,\n",
       "         0.03804766, -0.09236237],\n",
       "       [-0.10701666, -0.32589817, -0.02490966, ...,  0.11730062,\n",
       "         0.40794182,  0.2757084 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(wordEmb_file_bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news['title']\n",
    "    # 0        The Brands Queen Elizabeth, Prince Charles, an...\n",
    "    # 1        The Cost of Trump's Aid Freeze in the Trenches...\n",
    "\n",
    "# wordDictTokens\n",
    "    # [{'input_ids': tensor([[ 101, 1996, 9639, 3035, 3870, 1010, 3159, 2798, 1010, 1998, 3159, 5170,\n",
    "    #   8415, 2011,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
    "\n",
    "# decodedtokens\n",
    "    # ['[CLS] the brands queen elizabeth, prince charles, and prince philip swear by [SEP]',\n",
    "    #  \"[CLS] the cost of trump's aid freeze in the trenches of ukraine's war [SEP]\", ...]\n",
    "\n",
    "# tokens\n",
    "    # [['[CLS]', 'the', 'brands', 'queen', 'elizabeth', ',', 'prince', 'charles', ',', 'and', 'prince', 'philip', 'swear', 'by','[SEP]'],...]\n",
    "\n",
    "# tokenDF\n",
    "    # [        words\n",
    "    #  0       [CLS]\n",
    "    #  1         the\n",
    "    # ...       ...]\n",
    "\n",
    "# uniqTokens\n",
    "    #     words\n",
    "    # 0\t[CLS]\n",
    "    # 1\tthe\n",
    "    # 2\tbrands\n",
    "    # 3\tqueen\n",
    "    # 4\telizabeth\n",
    "    # ...    ...\n",
    "\n",
    "# tList\n",
    "    # ['[CLS]', 'the', 'brands', 'queen', 'elizabeth',',','prince','charles',',','and','prince','philip','swear','by','[SEP]', ...]\n",
    "\n",
    "# wordDict\n",
    "    # [{'[CLS]': 1}, {'the': 2}, ...]\n",
    "\n",
    "# token_ids\n",
    "    # [[101],[1996],[9639],[3035],[3870],[1010],[3159],[2798],[1010],[1998],[3159],[5170],[8415],[2011],[102],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encoded_input  = []\n",
    "\n",
    "# for o, title in enumerate(tqdm(news['title'])):\n",
    "#     tokenInputes = tokenizer(\n",
    "#         str(title),\n",
    "#         truncation=True,\n",
    "#         padding=True,\n",
    "#         return_tensors='pt',\n",
    "#         max_length=768) \n",
    "#     encoded_input = model(**tokenInputes)\n",
    "#     for p, word in enumerate(title):\n",
    "#         print( encoded_input.last_hidden_state[0][p].shape)\n",
    "#         np.savetxt(wordEmb_file_bert, [{word: encoded_input['last_hidden_state'][0][p].detach().numpy()}])\n",
    "#     if i == 0:\n",
    "#         # print(title, encoded_input.last_hidden_state[0].shape)\n",
    "#         break\n",
    "\n",
    "# # embTokens = {}\n",
    "# # for o, token in enumerate(tqdm(tokens)):\n",
    "# #     for p, word in enumerate(token):\n",
    "# #         embTokens[word] = encoded_input[o].last_hidden_state[0][p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokens = []\n",
    "# # for i, title in enumerate(tqdm(len(news['title']))):\n",
    "# #     tokens.append(tokenizer.tokenize(str(news['title'][i])))\n",
    "\n",
    "# tokenDF = []\n",
    "# for i in tqdm(range(len(news['title']))):\n",
    "#     tokenDF.append(pd.DataFrame(tokens[i], columns=['words']))\n",
    "\n",
    "# uniqTokens = pd.concat(tokenDF).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# tList= uniqTokens['words'].values.tolist()\n",
    "# wordDict = {}\n",
    "# for i, word in enumerate(tqdm(tList)):\n",
    "#     wordDict[word] = i+1\n",
    "\n",
    "# token_ids = []\n",
    "# for i, word in enumerate(tqdm(wordDict)):\n",
    "#     token_ids.append(tokenizer.convert_tokens_to_ids(word))\n",
    "\n",
    "# with open(wordDict_file, 'wb') as handle:\n",
    "#     pickle.dump(wordDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decodedtokens = []\n",
    "# for i in tqdm(range(len(wordDictTokens))):\n",
    "#     decodedtokens.append(tokenizer.decode(wordDictTokens[i]['input_ids'][0].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertForPreTraining\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "# # inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# outputs = model(**wordDictTokens[0])\n",
    "\n",
    "# prediction_logits = outputs.prediction_logits\n",
    "# seq_relationship_logits = outputs.seq_relationship_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_input  = np.zeros((len(token_ids)), dtype=object)\n",
    "# for i, title in enumerate(tqdm(token_ids)):\n",
    "#     print(i, title)\n",
    "#     if i == 5:\n",
    "#         break\n",
    "#     encoded_input[i] = model([])\n",
    "# # #     BERT_tokens[i] = tokenizer.tokenize(title)\n",
    "# # len(wordDict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a48c56cda2aee02d02b47f73c4873c502d3c354ec69b58d08b5e9de7ab71483b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('Recommenders': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
