{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy\n",
    "# ! pip install zipfile36\n",
    "# ! pip install pandas\n",
    "# ! pip install tqdm\n",
    "# ! pip install ipywidgets\n",
    "# ! pip install scrapbook\n",
    "# ! pip install tempfile\n",
    "# ! pip install tensorflow\n",
    "# ! pip install retrying\n",
    "# ! pip install transformers\n",
    "# ! conda install numpy-base\n",
    "# ! pip install recommenders[npa,gpu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/papermill/iorw.py:50: FutureWarning: pyarrow.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  from pyarrow import HadoopFileSystem\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.7.10 (default, Jun  4 2021, 14:48:32) \n",
      "[GCC 7.5.0]\n",
      "Tensorflow version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "\n",
    "from reco_utils.recommender.deeprec.deeprec_utils import download_deeprec_resources\n",
    "from reco_utils.recommender.newsrec.newsrec_utils import get_mind_data_set\n",
    "from reco_utils.recommender.newsrec.io.mind_iterator import MINDIterator\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')  # only show error messages\n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "\n",
    "# Options: demo, small, large\n",
    "MIND_type = 'demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "# data_path = tmpdir.name\n",
    "data_path = os.path.join('datasets', MIND_type)\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
    "\n",
    "yaml_file = os.path.join(data_path, \"utils\", r'npa.yaml')\n",
    "\n",
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "\n",
    "if not os.path.exists(train_news_file):\n",
    "    download_deeprec_resources(mind_url, os.path.join(\n",
    "        data_path, 'train'), mind_train_dataset)\n",
    "\n",
    "if not os.path.exists(valid_news_file):\n",
    "    download_deeprec_resources(mind_url,\n",
    "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/newsrec/',\n",
    "                               os.path.join(data_path, 'utils'), mind_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = ['news_ID', 'category', 'SubCategory', 'title',\n",
    "               'abstract', 'URL', 'titleEntities', 'abstractEntities']\n",
    "df = pd.read_csv(train_news_file , sep='\\t',\n",
    "                 header=None, names=columnNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 300\n",
    "num_samples = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Cost of Trump's Aid Freeze in the Trenches of Ukraine's War\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31028, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .pkl\n",
    "# list word\n",
    "# np.load(wordDict_file, allow_pickle=True)\n",
    "\n",
    "\n",
    "# .npy\n",
    "# list vector\n",
    "a = np.load(wordEmb_file, allow_pickle=True)\n",
    "a.shape\n",
    "\n",
    "\n",
    "# (31028, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 26740/26740 [00:14<00:00, 1820.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "BERT_tokens = np.zeros((num_samples, seq_length))\n",
    "x = 0\n",
    "encoded_input  = np.zeros((df['title'].count()), dtype=object)\n",
    "for i, title in enumerate(tqdm(df['title'])):\n",
    "    encoded_input[i] = torch.tensor([tokenizer.encode(title)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26740 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/26740 [00:00<1:06:54,  6.66it/s]\u001b[A\n",
      "  0%|          | 2/26740 [00:00<1:03:47,  6.99it/s]\u001b[A\n",
      "  0%|          | 3/26740 [00:00<1:01:35,  7.23it/s]\u001b[A\n",
      "  0%|          | 4/26740 [00:00<59:27,  7.49it/s]  \u001b[A\n",
      "  0%|          | 5/26740 [00:00<58:46,  7.58it/s]\u001b[A\n",
      "  0%|          | 6/26740 [00:00<1:01:26,  7.25it/s]\u001b[A\n",
      "  0%|          | 7/26740 [00:00<1:01:17,  7.27it/s]\u001b[A\n",
      "  0%|          | 8/26740 [00:01<1:02:23,  7.14it/s]\u001b[A\n",
      "  0%|          | 9/26740 [00:01<1:00:41,  7.34it/s]\u001b[A\n",
      "  0%|          | 10/26740 [00:01<1:06:51,  6.66it/s][A\n"
     ]
    }
   ],
   "source": [
    "outShape = np.zeros((df['title'].count()), dtype=object)\n",
    "with open('bert.npy', 'wb') as f:\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(df['title'].count())):\n",
    "            output = model(encoded_input[i])\n",
    "            np.save(f, np.array(output['last_hidden_state'][0].detach().numpy()), allow_pickle=True)\n",
    "            if i == 10:\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8991, -0.5201, -0.9452,  0.7746,  0.7234, -0.3744,  0.8507,  0.3547,\n",
       "         -0.8930, -1.0000, -0.5521,  0.9291,  0.9784,  0.7049,  0.9283, -0.7519,\n",
       "         -0.5475, -0.5965,  0.3389, -0.4499,  0.7782,  1.0000, -0.2826,  0.4853,\n",
       "          0.6410,  0.9842, -0.7867,  0.9369,  0.9399,  0.7433, -0.6914,  0.4468,\n",
       "         -0.9900, -0.2625, -0.9363, -0.9865,  0.4843, -0.6320,  0.0523,  0.0051,\n",
       "         -0.8996,  0.3441,  1.0000, -0.3589,  0.3788, -0.3925, -1.0000,  0.3703,\n",
       "         -0.8588,  0.9369,  0.9079,  0.9614,  0.2904,  0.5589,  0.5659, -0.6404,\n",
       "         -0.0192,  0.2399, -0.3038, -0.6871, -0.7484,  0.3960, -0.8917, -0.8742,\n",
       "          0.9562,  0.8571, -0.2774, -0.4376, -0.3216,  0.1390,  0.8761,  0.3578,\n",
       "         -0.2952, -0.8451,  0.8222,  0.4438, -0.7001,  1.0000, -0.5985, -0.9794,\n",
       "          0.9103,  0.8466,  0.6952, -0.6277,  0.7089, -1.0000,  0.6830, -0.1831,\n",
       "         -0.9875,  0.4217,  0.7032, -0.3897,  0.7486,  0.6686, -0.6870, -0.6460,\n",
       "         -0.4157, -0.8516, -0.3705, -0.6079,  0.2582, -0.3319, -0.5103, -0.4356,\n",
       "          0.4652, -0.6024, -0.4063,  0.6820,  0.4859,  0.7354,  0.5064, -0.4281,\n",
       "          0.6386, -0.9368,  0.7184, -0.4642, -0.9877, -0.6607, -0.9921,  0.6069,\n",
       "         -0.3670, -0.4372,  0.9294, -0.6055,  0.3951, -0.2804, -0.9368, -1.0000,\n",
       "         -0.7596, -0.4692, -0.5266, -0.4477, -0.9752, -0.9658,  0.6990,  0.9548,\n",
       "          0.3533,  0.9999, -0.4152,  0.9141, -0.4500, -0.7375,  0.7490, -0.5883,\n",
       "          0.7857, -0.0016, -0.5513,  0.2938, -0.4065,  0.2409, -0.8407, -0.4035,\n",
       "         -0.8404, -0.9050, -0.3938,  0.9415, -0.6345, -0.9703, -0.4040, -0.4583,\n",
       "         -0.4861,  0.7679,  0.8099,  0.5246, -0.5446,  0.5753,  0.2786,  0.5707,\n",
       "         -0.8072, -0.2453,  0.5690, -0.4669, -0.8838, -0.9827, -0.5031,  0.4506,\n",
       "          0.9870,  0.7017,  0.3622,  0.8111, -0.4196,  0.6760, -0.9451,  0.9802,\n",
       "         -0.2075,  0.3001, -0.5552,  0.7859, -0.7941,  0.0742,  0.8527, -0.7106,\n",
       "         -0.7577, -0.2244, -0.5434, -0.5502, -0.8017,  0.3969, -0.4385, -0.3792,\n",
       "         -0.1288,  0.9139,  0.9793,  0.7335,  0.5692,  0.6611, -0.9094, -0.4188,\n",
       "          0.2957,  0.3493,  0.2594,  0.9926, -0.7356, -0.2965, -0.8941, -0.9853,\n",
       "          0.1076, -0.8696, -0.2956, -0.7121,  0.7359, -0.4935,  0.5549,  0.3387,\n",
       "         -0.9495, -0.7334,  0.5352, -0.5096,  0.5660, -0.4018,  0.8542,  0.9659,\n",
       "         -0.7054,  0.7295,  0.9319, -0.9387, -0.7811,  0.8010, -0.4873,  0.7813,\n",
       "         -0.6892,  0.9826,  0.9063,  0.7559, -0.8902, -0.8375, -0.8407, -0.7466,\n",
       "         -0.1373,  0.2860,  0.9118,  0.6919,  0.5529,  0.0477, -0.5938,  0.9944,\n",
       "         -0.8876, -0.9638, -0.7532, -0.3716, -0.9877,  0.8710,  0.4599,  0.5771,\n",
       "         -0.6157, -0.6527, -0.9600,  0.8748,  0.2301,  0.9830, -0.5873, -0.9177,\n",
       "         -0.7872, -0.9336,  0.0299, -0.3313, -0.6276,  0.0359, -0.9386,  0.6413,\n",
       "          0.5652,  0.5971, -0.9518,  0.9989,  1.0000,  0.9608,  0.8791,  0.7950,\n",
       "         -1.0000, -0.6845,  1.0000, -0.9943, -1.0000, -0.9323, -0.6953,  0.3987,\n",
       "         -1.0000, -0.3411, -0.0337, -0.8604,  0.8336,  0.9708,  0.9905, -1.0000,\n",
       "          0.7739,  0.9286, -0.6900,  0.9583, -0.5780,  0.9719,  0.6315,  0.6815,\n",
       "         -0.3709,  0.5687, -0.9707, -0.8550, -0.6948, -0.8755,  0.9992,  0.2968,\n",
       "         -0.7734, -0.8658,  0.5569, -0.3101, -0.0184, -0.9617, -0.3699,  0.6742,\n",
       "          0.7658,  0.2366,  0.2493, -0.5654,  0.4375,  0.4121,  0.1151,  0.7459,\n",
       "         -0.9072, -0.5579, -0.4958,  0.3205, -0.8004, -0.9666,  0.9572, -0.4315,\n",
       "          0.8995,  1.0000,  0.4259, -0.8240,  0.6463,  0.4952, -0.2002,  1.0000,\n",
       "          0.8282, -0.9734, -0.6799,  0.6798, -0.7107, -0.7620,  0.9992, -0.4710,\n",
       "         -0.8334, -0.4501,  0.9830, -0.9890,  0.9977, -0.7832, -0.9556,  0.9544,\n",
       "          0.9291, -0.6558, -0.4738,  0.2732, -0.7858,  0.4154, -0.9412,  0.8092,\n",
       "          0.5430, -0.2157,  0.8470, -0.7974, -0.7055,  0.4348, -0.8898, -0.4324,\n",
       "          0.9173,  0.5623, -0.1846,  0.1917, -0.4493, -0.5846, -0.9617,  0.6340,\n",
       "          1.0000, -0.4347,  0.8515, -0.5470, -0.1285,  0.0894,  0.6514,  0.6111,\n",
       "         -0.3319, -0.7450,  0.7956, -0.9580, -0.9918,  0.7376,  0.3845, -0.3220,\n",
       "          1.0000,  0.5608,  0.3949,  0.5289,  0.9893,  0.0564,  0.4044,  0.9381,\n",
       "          0.9819, -0.4421,  0.7359,  0.7909, -0.9466, -0.3404, -0.6612,  0.2108,\n",
       "         -0.9263, -0.1385, -0.9307,  0.9573,  0.9806,  0.5332,  0.2399,  0.7653,\n",
       "          1.0000, -0.7914,  0.6762, -0.2680,  0.7779, -0.9999, -0.7669, -0.5375,\n",
       "         -0.1925, -0.8865, -0.3619,  0.3402, -0.9569,  0.8661,  0.6807, -0.9769,\n",
       "         -0.9865, -0.6337,  0.8399,  0.2236, -0.9906, -0.8362, -0.5692,  0.5195,\n",
       "         -0.4240, -0.8970, -0.4352, -0.4736,  0.6400, -0.3549,  0.6750,  0.9171,\n",
       "          0.6837, -0.7963, -0.5776, -0.2890, -0.8546,  0.8525, -0.8509, -0.9420,\n",
       "         -0.2516,  1.0000, -0.5016,  0.9301,  0.6883,  0.7339, -0.4419,  0.2935,\n",
       "          0.9499,  0.3775, -0.8626, -0.9399, -0.5290, -0.4906,  0.5731,  0.7014,\n",
       "          0.7417,  0.7273,  0.7713,  0.5002, -0.1915,  0.1372,  0.9994, -0.3031,\n",
       "         -0.2252, -0.6142, -0.2768, -0.5332, -0.2483,  1.0000,  0.4433,  0.6471,\n",
       "         -0.9904, -0.8510, -0.8769,  1.0000,  0.8798, -0.6752,  0.7479,  0.6240,\n",
       "         -0.4186,  0.7526, -0.2646, -0.2746,  0.3257,  0.2658,  0.9447, -0.6884,\n",
       "         -0.9693, -0.6836,  0.5302, -0.9496,  1.0000, -0.6971, -0.3515, -0.3904,\n",
       "         -0.4012,  0.2711,  0.2154, -0.9733, -0.3178,  0.4122,  0.9472,  0.3367,\n",
       "         -0.7153, -0.8901,  0.9290,  0.7933, -0.9311, -0.8989,  0.9562, -0.9659,\n",
       "          0.7023,  1.0000,  0.4563,  0.3897,  0.3707, -0.5580,  0.5297, -0.3703,\n",
       "          0.6910, -0.9349, -0.5441, -0.4148,  0.4783, -0.2760, -0.6926,  0.4691,\n",
       "          0.3460, -0.6889, -0.6922, -0.2883,  0.5287,  0.9148, -0.3298, -0.3370,\n",
       "          0.3139, -0.2623, -0.9256, -0.4132, -0.5292, -1.0000,  0.7196, -1.0000,\n",
       "          0.5927,  0.2626, -0.3615,  0.8260,  0.6851,  0.7507, -0.6682, -0.9576,\n",
       "          0.2358,  0.8092, -0.3510, -0.4920, -0.6665,  0.4409, -0.1648,  0.2033,\n",
       "         -0.7707,  0.7886, -0.3540,  1.0000,  0.3350, -0.6834, -0.9630,  0.3828,\n",
       "         -0.4485,  1.0000, -0.8681, -0.9655,  0.3128, -0.8266, -0.7758,  0.5257,\n",
       "          0.1956, -0.7392, -0.9455,  0.9282,  0.9236, -0.7143,  0.6726, -0.4129,\n",
       "         -0.5943,  0.1813,  0.9067,  0.9815,  0.5940,  0.8806, -0.5073, -0.2626,\n",
       "          0.9598,  0.4193,  0.3074,  0.2767,  1.0000,  0.5056, -0.8592,  0.1262,\n",
       "         -0.9610, -0.2947, -0.9361,  0.4443,  0.4438,  0.8899, -0.3358,  0.9626,\n",
       "         -0.8515,  0.0794, -0.6963, -0.5930,  0.5559, -0.9107, -0.9782, -0.9843,\n",
       "          0.6980, -0.4755, -0.3016,  0.3066,  0.2964,  0.4888,  0.5341, -1.0000,\n",
       "          0.9480,  0.4918,  0.9318,  0.9587,  0.8098,  0.6707,  0.3939, -0.9838,\n",
       "         -0.9703, -0.4679, -0.3369,  0.7440,  0.6803,  0.8288,  0.4960, -0.6034,\n",
       "         -0.5991, -0.6975, -0.8558, -0.9912,  0.4839, -0.7709, -0.9521,  0.9518,\n",
       "          0.0340, -0.1954, -0.2836, -0.9258,  0.9254,  0.7029,  0.2994,  0.1614,\n",
       "          0.5028,  0.8677,  0.9138,  0.9789, -0.9067,  0.7399, -0.8542,  0.5366,\n",
       "          0.8467, -0.9445,  0.3244,  0.6015, -0.5501,  0.3863, -0.4039, -0.9572,\n",
       "          0.5761, -0.2973,  0.4641, -0.4283,  0.0236, -0.4914, -0.3320, -0.5634,\n",
       "         -0.6864,  0.6878,  0.3901,  0.8710,  0.8477, -0.2120, -0.6866, -0.3050,\n",
       "         -0.8349, -0.8857,  0.8521, -0.0979, -0.4760,  0.8032,  0.0689,  0.8095,\n",
       "          0.3115, -0.4875, -0.4914, -0.7535,  0.8345, -0.6730, -0.7538, -0.6694,\n",
       "          0.5267,  0.4821,  1.0000, -0.8562, -0.9474, -0.5897, -0.5382,  0.4599,\n",
       "         -0.4969, -1.0000,  0.4086, -0.7584,  0.7383, -0.7259,  0.8153, -0.8347,\n",
       "         -0.9768, -0.4615,  0.7301,  0.8539, -0.5872, -0.7596,  0.6707, -0.6158,\n",
       "          0.9688,  0.8280, -0.5608, -0.1609,  0.7421, -0.8494, -0.7306,  0.8998]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/31027 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 3 to array axis with dimension 768",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7e408ad0ca07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     [CLS] word [SEP]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mBERT_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot copy sequence with size 3 to array axis with dimension 768"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "BERT_tokens = np.zeros((len(df), 768),dtype=object)\n",
    "# encoded_input  = np.zeros((df['title'].count()), dtype=object)\n",
    "for i, word in enumerate(tqdm(df)):\n",
    "#     [CLS] word [SEP]\n",
    "    BERT_tokens[i] = tokenizer(word, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertModel object argument after ** must be a mapping, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-554dbf224ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#         for i in tqdm(range(len(df))):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mBERT_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#         if i == 10:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: BertModel object argument after ** must be a mapping, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "outShape = np.zeros((len(df)), dtype=object)\n",
    "bert_embedding_file = os.path.join(data_path, \"utils\", \"bert_embedding.npy\")\n",
    "with open(bert_embedding_file, 'wb') as f:\n",
    "    with torch.no_grad():\n",
    "#         for i in tqdm(range(len(df))):\n",
    "        output = model(**BERT_tokens)\n",
    "        np.save(f, np.array(output['last_hidden_state'][0].detach().numpy()), allow_pickle=True)\n",
    "#         if i == 10:\n",
    "#             break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1386,  0.1583, -0.2967,  ..., -0.2709, -0.2844,  0.4581],\n",
       "         [ 0.5364, -0.2327,  0.1754,  ...,  0.5540,  0.4981, -0.0024],\n",
       "         [ 0.3002, -0.3475,  0.1208,  ..., -0.4562,  0.3288,  0.8773],\n",
       "         ...,\n",
       "         [ 0.3799,  0.1203,  0.8283,  ..., -0.8624, -0.5957,  0.0471],\n",
       "         [-0.0252, -0.7177, -0.6950,  ...,  0.0757, -0.6668, -0.3401],\n",
       "         [ 0.7535,  0.2391,  0.0717,  ...,  0.2467, -0.6458, -0.3213]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.9377, -0.5043, -0.9799,  0.9030,  0.9329, -0.2438,  0.8926,  0.2288,\n",
       "         -0.9531, -1.0000, -0.8862,  0.9906,  0.9855,  0.7155,  0.9455, -0.8645,\n",
       "         -0.6035, -0.6666,  0.3020, -0.1587,  0.7455,  1.0000, -0.4022,  0.4261,\n",
       "          0.6151,  0.9996, -0.8773,  0.9594,  0.9585,  0.6950, -0.6718,  0.3325,\n",
       "         -0.9954, -0.2268, -0.9658, -0.9951,  0.6127, -0.7670,  0.0873,  0.0824,\n",
       "         -0.9518,  0.4713,  1.0000,  0.3299,  0.7583, -0.2670, -1.0000,  0.3166,\n",
       "         -0.9364,  0.9910,  0.9719,  0.9893,  0.2190,  0.6048,  0.5849, -0.4123,\n",
       "         -0.0063,  0.1719, -0.3988, -0.6190, -0.6603,  0.5069, -0.9757, -0.9039,\n",
       "          0.9926,  0.9323, -0.3687, -0.4869, -0.3143,  0.0499,  0.9129,  0.3396,\n",
       "         -0.1879, -0.9235,  0.8675,  0.3228, -0.6406,  1.0000, -0.7989, -0.9931,\n",
       "          0.9629,  0.9124,  0.4827, -0.7276,  0.5996, -1.0000,  0.7548, -0.1600,\n",
       "         -0.9941,  0.3386,  0.8394, -0.4158,  0.2943,  0.6111, -0.5745, -0.7185,\n",
       "         -0.4768, -0.9681, -0.4327, -0.6732,  0.1248, -0.2093, -0.5882, -0.4186,\n",
       "          0.5447, -0.6125, -0.6138,  0.4712,  0.4779,  0.7633,  0.3974, -0.4148,\n",
       "          0.7063, -0.9680,  0.7389, -0.4270, -0.9948, -0.6019, -0.9950,  0.7459,\n",
       "         -0.6343, -0.2753,  0.9522, -0.5724,  0.6218, -0.1295, -0.9905, -1.0000,\n",
       "         -0.8710, -0.7506, -0.5008, -0.4827, -0.9872, -0.9847,  0.7214,  0.9694,\n",
       "          0.3013,  1.0000, -0.4427,  0.9699, -0.5431, -0.8189,  0.9180, -0.5132,\n",
       "          0.9026,  0.5274, -0.5940,  0.2928, -0.6933,  0.7179, -0.9318, -0.2776,\n",
       "         -0.9160, -0.9457, -0.3287,  0.9556, -0.7927, -0.9860, -0.1904, -0.2760,\n",
       "         -0.6062,  0.9005,  0.9266,  0.4353, -0.6858,  0.4720,  0.2851,  0.7685,\n",
       "         -0.8647, -0.5626,  0.5127, -0.5468, -0.9490, -0.9907, -0.5809,  0.7146,\n",
       "          0.9948,  0.7981,  0.3463,  0.9349, -0.4238,  0.9333, -0.9754,  0.9936,\n",
       "         -0.2597,  0.4665, -0.5400,  0.4947, -0.8723,  0.0034,  0.8378, -0.9134,\n",
       "         -0.8432, -0.2516, -0.5177, -0.4687, -0.9491,  0.5691, -0.4856, -0.4857,\n",
       "         -0.2245,  0.9609,  0.9823,  0.7496,  0.6256,  0.8552, -0.9073, -0.5802,\n",
       "          0.2874,  0.3017,  0.3016,  0.9974, -0.8503, -0.2108, -0.9261, -0.9907,\n",
       "         -0.0252, -0.9488, -0.3972, -0.8097,  0.8707, -0.7512,  0.8107,  0.5488,\n",
       "         -0.9830, -0.8569,  0.4852, -0.6156,  0.4846, -0.2893,  0.9647,  0.9858,\n",
       "         -0.7064,  0.7120,  0.9593, -0.9590, -0.8708,  0.7893, -0.3561,  0.8603,\n",
       "         -0.7243,  0.9882,  0.9876,  0.9282, -0.9547, -0.8329, -0.7993, -0.8398,\n",
       "         -0.2333,  0.2315,  0.9712,  0.6055,  0.6388,  0.2429, -0.7884,  0.9981,\n",
       "         -0.9448, -0.9804, -0.8184, -0.3534, -0.9951,  0.9729,  0.4165,  0.8094,\n",
       "         -0.6227, -0.8183, -0.9817,  0.8532,  0.1242,  0.9826, -0.6376, -0.9450,\n",
       "         -0.8094, -0.9748,  0.0412, -0.3097, -0.8153, -0.0306, -0.9255,  0.5677,\n",
       "          0.6217,  0.6652, -0.9682,  0.9997,  1.0000,  0.9826,  0.9013,  0.8950,\n",
       "         -1.0000, -0.8081,  1.0000, -0.9995, -1.0000, -0.9361, -0.8200,  0.4755,\n",
       "         -1.0000, -0.2698, -0.0111, -0.9297,  0.8492,  0.9879,  0.9950, -1.0000,\n",
       "          0.8653,  0.9513, -0.5679,  0.9966, -0.6713,  0.9815,  0.6008,  0.7414,\n",
       "         -0.3265,  0.5574, -0.9801, -0.8956, -0.8082, -0.9267,  0.9999,  0.2542,\n",
       "         -0.7970, -0.8854,  0.7831, -0.1391, -0.0060, -0.9786, -0.4503,  0.8895,\n",
       "          0.9021,  0.3021,  0.2650, -0.5750,  0.5099,  0.1216,  0.1170,  0.6484,\n",
       "         -0.9505, -0.3889, -0.6938,  0.2508, -0.7526, -0.9831,  0.9646, -0.2742,\n",
       "          0.9865,  1.0000,  0.3756, -0.9045,  0.8847,  0.4860, -0.5515,  1.0000,\n",
       "          0.9092, -0.9904, -0.4959,  0.7900, -0.7156, -0.8280,  0.9999, -0.4197,\n",
       "         -0.9282, -0.7733,  0.9945, -0.9956,  0.9998, -0.8985, -0.9838,  0.9735,\n",
       "          0.9655, -0.8103, -0.8325,  0.1020, -0.6722,  0.4561, -0.9412,  0.8396,\n",
       "          0.6979, -0.1201,  0.9288, -0.8345, -0.6312,  0.4356, -0.8901, -0.4565,\n",
       "          0.9874,  0.5709, -0.2111, -0.0206, -0.4182, -0.9116, -0.9781,  0.8246,\n",
       "          1.0000, -0.4229,  0.9489, -0.5226, -0.0986,  0.2202,  0.7459,  0.7152,\n",
       "         -0.3528, -0.8800,  0.9299, -0.9716, -0.9949,  0.7278,  0.2206, -0.4944,\n",
       "          1.0000,  0.6285,  0.3795,  0.7228,  0.9993,  0.0301,  0.5936,  0.9816,\n",
       "          0.9914, -0.3465,  0.5882,  0.8365, -0.9824, -0.4488, -0.7612,  0.1331,\n",
       "         -0.9479, -0.0559, -0.9697,  0.9846,  0.9960,  0.5818,  0.3121,  0.8577,\n",
       "          1.0000, -0.9274,  0.6693, -0.1365,  0.8035, -1.0000, -0.8057, -0.4504,\n",
       "         -0.1711, -0.9512, -0.5899,  0.3991, -0.9754,  0.9563,  0.8806, -0.9937,\n",
       "         -0.9923, -0.4979,  0.8853,  0.1439, -0.9994, -0.8986, -0.6272,  0.8385,\n",
       "         -0.3239, -0.9470, -0.7009, -0.4768,  0.5742, -0.2216,  0.5665,  0.9667,\n",
       "          0.7935, -0.9401, -0.6746, -0.1753, -0.9163,  0.9409, -0.8701, -0.9894,\n",
       "         -0.2514,  1.0000, -0.4087,  0.9385,  0.6050,  0.8219, -0.2712,  0.3326,\n",
       "          0.9827,  0.3613, -0.8314, -0.9850, -0.2861, -0.5398,  0.8254,  0.8414,\n",
       "          0.7590,  0.9412,  0.9627,  0.2765, -0.0737,  0.0399,  0.9998, -0.3095,\n",
       "         -0.1933, -0.4689, -0.2511, -0.4629, -0.2914,  1.0000,  0.3963,  0.7777,\n",
       "         -0.9950, -0.9808, -0.9303,  1.0000,  0.8822, -0.6848,  0.8124,  0.6242,\n",
       "         -0.2551,  0.8266, -0.2791, -0.3167,  0.2294,  0.1682,  0.9627, -0.6738,\n",
       "         -0.9904, -0.7910,  0.7099, -0.9770,  1.0000, -0.7030, -0.3960, -0.5981,\n",
       "         -0.6683, -0.2727, -0.0183, -0.9882, -0.3841,  0.5605,  0.9745,  0.3505,\n",
       "         -0.4898, -0.9298,  0.9578,  0.9533, -0.9859, -0.9597,  0.9777, -0.9784,\n",
       "          0.7550,  1.0000,  0.3446,  0.6786,  0.3947, -0.5349,  0.5541, -0.6754,\n",
       "          0.8078, -0.9595, -0.4484, -0.3901,  0.3983, -0.1319, -0.2896,  0.7860,\n",
       "          0.3500, -0.5530, -0.7294, -0.2361,  0.4663,  0.9332, -0.3048, -0.1916,\n",
       "          0.2318, -0.3230, -0.9323, -0.4672, -0.6315, -1.0000,  0.8068, -1.0000,\n",
       "          0.8035,  0.4066, -0.3700,  0.8760,  0.7829,  0.8298, -0.8628, -0.9795,\n",
       "          0.1322,  0.8529, -0.5029, -0.9057, -0.6918,  0.5017, -0.2052,  0.1564,\n",
       "         -0.7397,  0.8156, -0.3414,  1.0000,  0.2659, -0.8292, -0.9821,  0.2491,\n",
       "         -0.3009,  1.0000, -0.8952, -0.9832,  0.3330, -0.9179, -0.8493,  0.5868,\n",
       "          0.1653, -0.8522, -0.9961,  0.9220,  0.8661, -0.6477,  0.7927, -0.3991,\n",
       "         -0.7691,  0.1512,  0.9868,  0.9924,  0.7317,  0.9083, -0.1226, -0.5258,\n",
       "          0.9840,  0.4009, -0.0436,  0.1361,  1.0000,  0.4004, -0.9497, -0.1309,\n",
       "         -0.9788, -0.3522, -0.9551,  0.3755,  0.3099,  0.9195, -0.4460,  0.9738,\n",
       "         -0.9714,  0.1901, -0.8894, -0.7863,  0.4757, -0.9463, -0.9892, -0.9938,\n",
       "          0.8142, -0.4077, -0.1895,  0.2102,  0.1715,  0.6322,  0.5566, -1.0000,\n",
       "          0.9642,  0.6150,  0.9768,  0.9768,  0.9115,  0.8108,  0.3251, -0.9920,\n",
       "         -0.9910, -0.5438, -0.3567,  0.7960,  0.7648,  0.8900,  0.6470, -0.4875,\n",
       "         -0.4792, -0.7756, -0.8423, -0.9972,  0.5961, -0.8679, -0.9678,  0.9718,\n",
       "         -0.3461, -0.1534, -0.2139, -0.9586,  0.9321,  0.7627,  0.4636,  0.0862,\n",
       "          0.5071,  0.9170,  0.9597,  0.9882, -0.9231,  0.8555, -0.9196,  0.6712,\n",
       "          0.9381, -0.9606,  0.2335,  0.8301, -0.5560,  0.3696, -0.4752, -0.9740,\n",
       "          0.8174, -0.4268,  0.7773, -0.4798,  0.0639, -0.4718, -0.2607, -0.7624,\n",
       "         -0.8742,  0.6576,  0.6207,  0.9219,  0.9360, -0.0496, -0.8942, -0.3701,\n",
       "         -0.8944, -0.9526,  0.9536, -0.0851, -0.2961,  0.9031,  0.1321,  0.9324,\n",
       "          0.4289, -0.4989, -0.4174, -0.7639,  0.8887, -0.7894, -0.7639, -0.7093,\n",
       "          0.8105,  0.3595,  1.0000, -0.9188, -0.9878, -0.8268, -0.6012,  0.4992,\n",
       "         -0.7880, -1.0000,  0.3609, -0.8314,  0.8524, -0.9398,  0.9500, -0.9339,\n",
       "         -0.9851, -0.3495,  0.8436,  0.9375, -0.5159, -0.8989,  0.5196, -0.8797,\n",
       "          0.9979,  0.8753, -0.8277, -0.0012,  0.6013, -0.9184, -0.7398,  0.9228]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "a48c56cda2aee02d02b47f73c4873c502d3c354ec69b58d08b5e9de7ab71483b"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
